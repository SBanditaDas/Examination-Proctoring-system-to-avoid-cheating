<p align="center">
  <img src="https://img.shields.io/badge/âš¡_EDGE_AI-Powered-blueviolet?style=for-the-badge" alt="Edge AI"/>
  <img src="https://img.shields.io/badge/Models-4_Neural_Networks-orange?style=for-the-badge&logo=tensorflow" alt="4 Models"/>
  <img src="https://img.shields.io/badge/Privacy-100%25_Client--Side-green?style=for-the-badge&logo=shield" alt="Privacy"/>
</p>

<h1 align="center">ğŸ›¡ï¸ Face Â· Object Â· Gaze<br/>AI Proctoring System</h1>

<p align="center">
  <strong>The "Super Brain" â€” A next-generation, zero-latency, browser-based proctoring engine<br/>that runs 4 neural networks simultaneously on the client side.</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/React-19-61DAFB?style=flat-square&logo=react&logoColor=white" alt="React"/>
  <img src="https://img.shields.io/badge/Vite-7-646CFF?style=flat-square&logo=vite&logoColor=white" alt="Vite"/>
  <img src="https://img.shields.io/badge/TensorFlow.js-4.22-FF6F00?style=flat-square&logo=tensorflow&logoColor=white" alt="TFjs"/>
  <img src="https://img.shields.io/badge/MediaPipe-FaceMesh-4285F4?style=flat-square&logo=google&logoColor=white" alt="MediaPipe"/>
  <img src="https://img.shields.io/badge/Tailwind_CSS-4-06B6D4?style=flat-square&logo=tailwindcss&logoColor=white" alt="Tailwind"/>
  <img src="https://img.shields.io/badge/License-MIT-purple?style=flat-square" alt="MIT"/>
</p>

---

## ğŸ¯ What Is This?

A sophisticated **React + Vite** application engineered to ensure the integrity of online examinations â€” entirely in the browser.

Unlike traditional proctoring tools that rely on backend video streaming (high latency, high cost, privacy concerns), this system leverages **Edge AI** to run **all processing on the user's device**. No video leaves the browser. No images are stored. The biometric embeddings exist only in RAM during the session.

---

## ğŸ§  The "Super Brain" â€” 4 Models, 1 Mission

Four neural networks load in parallel to create a comprehensive, real-time surveillance mesh:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ§  SUPER BRAIN ENGINE                    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MobileNetV2 â”‚  â”‚   COCO-SSD   â”‚  â”‚    BlazeFace     â”‚  â”‚
â”‚  â”‚  Identity ğŸ”’ â”‚  â”‚  Objects ğŸ“±  â”‚  â”‚  Head Count ğŸ‘¥   â”‚  â”‚
â”‚  â”‚  128-d embed â”‚  â”‚  Phone/Book  â”‚  â”‚  Multi-person    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              MediaPipe Face Mesh (468 pts)            â”‚   â”‚
â”‚  â”‚     ğŸ‘ï¸ Gaze Deviation    |    ğŸ‘„ Lip Movement        â”‚   â”‚
â”‚  â”‚     Iris ratio tracking  |    Talking detection      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| # | Model | Function | What It Detects | Technology |
|---|-------|----------|-----------------|------------|
| 1 | **MobileNetV2** | ğŸ”’ Identity Lock | Extracts 128-dimensional facial embeddings. Verifies the person sitting is the actual candidate using **Cosine Similarity**. | `tf.loadGraphModel` â€” Custom trained |
| 2 | **COCO-SSD** | ğŸ“± Object Scanner | Scans the room for unauthorized items: **cell phones**, **books**, and other forbidden objects in real-time. | `@tensorflow-models/coco-ssd` |
| 3 | **BlazeFace** | ğŸ‘¥ Head Counter | Detects if **multiple people** appear in the frame or if the user **leaves their seat** (no face detected). | `@mediapipe/face-detection` |
| 4 | **Face Mesh** | ğŸ‘ï¸ğŸ‘„ Behavior Analyzer | Tracks **468 facial landmarks** to detect **gaze deviation** (looking away from screen) and **lip movement** (talking/whispering). | `@mediapipe/face-mesh` |

---

## âœ¨ Feature Breakdown

### ğŸ” Phase 1 â€” Identity Calibration ("The Lock")

The system captures a high-quality baseline of the user's face and generates a unique **Tensor Vector** â€” essentially a biometric fingerprint stored in browser memory.

- Extracts a **128-dimensional embedding** from the webcam feed
- Locks this vector as the session's identity reference
- All future frames are compared against this baseline

### ğŸ›¡ï¸ Phase 2 â€” The Omni-Proctor Loop

Once locked, the system enters a continuous surveillance loop running at **~60 FPS**:

| Check | How It Works | Severity |
|-------|-------------|----------|
| **Identity Verification** | Calculates Cosine Similarity between live feed and locked baseline every frame | ğŸ”´ Critical |
| **Object Detection** | COCO-SSD scans for phones & books every 500ms | ğŸŸ¡ Warning |
| **Face Counting** | BlazeFace detects multiple faces or absence | ğŸ”´ Critical |
| **Gaze Tracking** | Iris-to-eye-corner ratio detects looking away | ğŸŸ¡ Warning |
| **Lip Movement** | Upper-lower lip distance detects talking | ğŸŸ¡ Warning |

#### ğŸ›¡ï¸ Robustness Layer â€” No False Positives

```
Strike System:  30 consecutive mismatched frames (~1 second) required to trigger alert
Cooldown Timer: 4-second gap between same-type alerts prevents spam
Throttling:     Heavy detections (objects, faces, gaze) run every 500ms, not every frame
```

### ğŸš« Phase 3 â€” Browser Lockdown & Auto-Termination

The system enforces strict browser security during the exam:

| Action | Response |
|--------|----------|
| **Tab Switch** (`Alt+Tab`, clicking another tab) | âŒ **Instant exam termination** |
| **Window Focus Loss** (clicking outside browser) | âŒ **Instant exam termination** |
| **Copy/Paste** (`Ctrl+C`, `Ctrl+V`) | ğŸš« Blocked & logged |
| **DevTools** (`F12`) | ğŸš« Blocked & logged |
| **Print Screen** | ğŸš« Blocked & logged |
| **Right-Click** | ğŸš« Disabled |

When tab switching or focus loss is detected, the exam is **immediately terminated** with a detailed incident report showing the violation type, timestamp, and unique incident ID.

### ğŸ“Š Phase 4 â€” Post-Exam Audit Dashboard

After submission, a comprehensive dashboard displays:

- **Total Violation Count** with severity breakdown (Critical vs Warning)
- **Detailed Audit Log** â€” timestamped table of every violation
- **Integrity Score** based on session behavior

---

## ğŸ—ï¸ Architecture & Tech Stack

```
src/
â”œâ”€â”€ App.jsx                          # Main application â€” UI + Surveillance Loop
â”œâ”€â”€ main.jsx                         # React entry point
â”œâ”€â”€ index.css                        # Tailwind CSS v4
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useProctorBrain.js           # ğŸ§  Loads all 4 AI models in parallel
â”‚   â””â”€â”€ useLockdown.js               # ğŸ”’ Tab-switch detection & browser lockdown
â”œâ”€â”€ components/
â”‚   â””â”€â”€ proctor/
â”‚       â””â”€â”€ Calibration.jsx          # Identity calibration UI
â””â”€â”€ assets/

public/
â””â”€â”€ models/
    â”œâ”€â”€ model.json                   # MobileNetV2 Face Recognition model manifest
    â”œâ”€â”€ group1-shard1of3.bin         # Model weights (4.0 MB)
    â”œâ”€â”€ group1-shard2of3.bin         # Model weights (4.0 MB)
    â””â”€â”€ group1-shard3of3.bin         # Model weights (1.1 MB)
```

| Layer | Technology |
|-------|-----------|
| **Frontend** | React 19, Vite 7 |
| **Styling** | Tailwind CSS v4 (with `@tailwindcss/vite` plugin) |
| **AI / ML** | TensorFlow.js 4.22, MediaPipe Face Detection & Face Mesh |
| **Object Detection** | COCO-SSD (MobileNet v2 backbone) |
| **Face Recognition** | Custom MobileNetV2 (128-d embeddings, ~9.2 MB) |
| **State Management** | React Hooks + `useRef` for performance-critical paths |
| **Math Engine** | Custom Cosine Similarity, Tensor operations |
| **Icons** | Lucide React |

---

## ğŸ“¦ Installation & Setup

### Prerequisites

- **Node.js** â‰¥ 18
- **npm** â‰¥ 9
- A device with a **webcam**
- A modern browser (Chrome/Edge recommended for WebGL support)

### Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/SBanditaDas/Face-Object-Gaze-Proctoring-Model.git
cd Face-Object-Gaze-Proctoring-Model

# 2. Install dependencies (fetches TF.js & MediaPipe binaries)
npm install

# 3. Launch the development server
npm run dev
```

Open **http://localhost:5173** and the Super Brain will begin initializing! ğŸ§ 

> **Note:** The first load may take a few seconds as TF.js downloads and initializes the COCO-SSD, BlazeFace, and Face Mesh models from CDNs. The Face Recognition model (9.2 MB) is bundled locally in `public/models/`.

---

## ğŸ® Usage Guide

### Step 1 â†’ Allow Camera Access
The app requires webcam permissions. Grant access when prompted.

### Step 2 â†’ Wait for AI Initialization
Watch for the button to change from *"Initializing AI..."* to *"Lock Identity & Start Exam"*. This means all 4 models are loaded and ready.

### Step 3 â†’ Lock Your Identity
Center your face in the webcam view and click **"Lock Identity & Start Exam"**. This captures your biometric baseline.

### Step 4 â†’ Take the Exam
The UI switches to exam mode with a live **Match Score** and **Audit Trail**:

| Try This | Expected Detection |
|----------|-------------------|
| Hold up a **phone** ğŸ“± | `UNAUTHORIZED_OBJECT: cell phone` |
| Hold up a **book** ğŸ“• | `UNAUTHORIZED_OBJECT: book` |
| **Look left/right** ğŸ‘€ | `LOOKING_AWAY_FROM_SCREEN` |
| **Talk or whisper** ğŸ—£ï¸ | `TALKING_DETECTED` |
| Have **another person** appear ğŸ‘¥ | `MULTIPLE_FACES_DETECTED` |
| **Leave the frame** ğŸš¶ | `NO_FACE_IN_FRAME` |
| **Switch tabs** âš ï¸ | `EXAM TERMINATED` â€” instant |
| **Cover your face** ğŸ™ˆ | `IDENTITY_MISMATCH` (after ~1 sec) |

### Step 5 â†’ End Session
Click **"End Exam"** to view the **Post-Exam Audit** dashboard with severity breakdown and full violation log.

---

## âš ï¸ Privacy & Security

<table>
  <tr>
    <td>âœ…</td>
    <td><strong>100% Client-Side Processing</strong> â€” All video analysis runs locally on the user's device</td>
  </tr>
  <tr>
    <td>âœ…</td>
    <td><strong>Zero Server Communication</strong> â€” No video feeds, images, or biometric data are ever transmitted</td>
  </tr>
  <tr>
    <td>âœ…</td>
    <td><strong>Session-Only Memory</strong> â€” Facial embeddings exist only in browser RAM and are destroyed on page close</td>
  </tr>
  <tr>
    <td>âœ…</td>
    <td><strong>No Database Storage</strong> â€” No facial images or vectors are persisted anywhere</td>
  </tr>
</table>

---

## ğŸ”§ Configuration & Tuning

Key thresholds can be adjusted in `App.jsx`:

| Parameter | Current Value | Location | Description |
|-----------|--------------|----------|-------------|
| Identity Threshold | `0.75` | Line ~133 | Cosine similarity below this = mismatch |
| Strike Threshold | `30 frames` | Line ~140 | Consecutive mismatches needed to trigger alert |
| Mismatch Cooldown | `4000ms` | Line ~140 | Gap between identity mismatch alerts |
| Detection Interval | `500ms` | Line ~147 | How often object/face/gaze checks run |
| Lip Open Threshold | `5px` | Line ~180 | Vertical lip distance to detect talking |
| Talking Cooldown | `2000ms` | Line ~180 | Gap between talking alerts |
| Gaze Deviation | `0.30 / 0.70` | Line ~202 | Iris ratio bounds for "looking away" |
| Gaze Cooldown | `2000ms` | Line ~202 | Gap between gaze alerts |

---

## ğŸ¤ Contributing

Contributions are welcome! Please fork this repository and submit a Pull Request.

```bash
# 1. Fork the Project
# 2. Create your Feature Branch
git checkout -b feature/AmazingFeature

# 3. Commit your Changes
git commit -m 'Add some AmazingFeature'

# 4. Push to the Branch
git push origin feature/AmazingFeature

# 5. Open a Pull Request
```

---

## ğŸ“„ License

Distributed under the **MIT License**. See `LICENSE` for more information.

---

<div align="center">
  <br/>
  <strong>Built with â¤ï¸ by S Bandita Das</strong>
  <br/>
  <sub>Powered by React Â· TensorFlow.js Â· MediaPipe</sub>
  <br/><br/>
  <img src="https://img.shields.io/badge/Made_with-React_&_TensorFlow.js-blue?style=for-the-badge" alt="Made with"/>
</div>
